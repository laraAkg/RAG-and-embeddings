{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a FAISS-Based Vector Store: A Journey Through Data Processing and Visualization\n",
    "\n",
    "In this notebook, you'll learn how to transform raw PDF documents into a searchable vector store using FAISS. We'll go on a journey where we:\n",
    "\n",
    "1. **Read and extract text from PDF files.**\n",
    "2. **Split the text into manageable chunks.**\n",
    "3. **Display tokenization outputs from different tokenizers.**\n",
    "4. **Generate embeddings from the text using a SentenceTransformer.**\n",
    "5. **Store the embeddings in a FAISS index.**\n",
    "6. **Project the embeddings into 2D space using UMAP for visualization.**\n",
    "7. **Visualize the entire process on a scatter plot.**\n",
    "8. **Incect your data into a prompt for a large language model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # For generating embeddings for text chunks\n",
    "import faiss\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv  # For loading environment variables from a .env file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading Data from PDFs\n",
    "\n",
    "First, we load PDF files from a directory, extract their text content, and combine it into one large text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl geladener PDFs: 1\n",
      "Gesamtlänge des extrahierten Texts: 48630 Zeichen\n",
      "High and normal protein diets improve body composition and \n",
      "glucose control in adults with type 2 diabetes: A randomized \n",
      "trial\n",
      "Julianne G. Clina1, R. Drew Sayer1,3, Zhaoxing Pan2, Caroline W. Cohen3, Michael T. \n",
      "McDermott4, Victoria A. Catenacci4, Holly R. Wyatt1,5, James O. Hill1\n",
      "1Department of Nutrition Sciences, University of Alabama at Birmingham\n",
      "2Department of Pediatrics, University of Colorado Anschutz Medical Campus\n",
      "3Department of Family and Community Medicine, University of Alabama at Birmingham\n",
      "4Division of Endocrinology, Metabolism and Diabetes, University of Colorado School of Medicine, \n",
      "Aurora, Colorado\n",
      "5Anschutz Health and Wellness Center, University of Colorado Anschutz Medical Campus\n",
      "Abstract\n",
      "Objective:  Weight loss of ≥10% improves glucose control and may remit type 2 diabetes \n",
      "(T2D). High protein (HP) diets are commonly used for weight loss, but whether protein \n",
      "sources, especially red meat, impact weight loss-induced T2D management is unknown. This \n",
      "trial compared a \n"
     ]
    }
   ],
   "source": [
    "# Alle PDFs aus dem Verzeichnis \"data\" laden\n",
    "pdf_files = glob.glob(\"data/*.pdf\")  # z. B. \"data/nihms-*.pdf\"\n",
    "\n",
    "# Inhalte aller PDFs extrahieren und in einem String sammeln\n",
    "all_text = \"\"\n",
    "\n",
    "for pdf_path in pdf_files:\n",
    "    reader = PdfReader(pdf_path)\n",
    "    for page in reader.pages:\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            all_text += text + \"\\n\"\n",
    "\n",
    "# Ergebnis prüfen\n",
    "print(f\"Anzahl geladener PDFs: {len(pdf_files)}\")\n",
    "print(f\"Gesamtlänge des extrahierten Texts: {len(all_text)} Zeichen\")\n",
    "print(all_text[:1000])  # Vorschau: erste 1000 Zeichen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "## 2. Splitting the Text into Chunks\n",
    "\n",
    "Large texts can be difficult to work with. We use a text splitter to break the full text into smaller, overlapping chunks. This helps preserve context when we later embed the text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Chunks: 28\n",
      "Beispiel-Chunk:\n",
      "High and normal protein diets improve body composition and \n",
      "glucose control in adults with type 2 diabetes: A randomized \n",
      "trial\n",
      "Julianne G. Clina1, R. Drew Sayer1,3, Zhaoxing Pan2, Caroline W. Cohen3, Michael T. \n",
      "McDermott4, Victoria A. Catenacci4, Holly R. Wyatt1,5, James O. Hill1\n",
      "1Department of Nu...\n"
     ]
    }
   ],
   "source": [
    "# Konfiguration: 2000 Zeichen pro Chunk, 200 Zeichen Überlappung\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "# Text zerschneiden\n",
    "chunks = splitter.split_text(all_text)\n",
    "\n",
    "# Ergebnis prüfen\n",
    "print(f\"Anzahl Chunks: {len(chunks)}\")\n",
    "print(f\"Beispiel-Chunk:\\n{chunks[0][:300]}...\")  # zeige die ersten 300 Zeichen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 28\n",
      "Preview of the first chunk: High and normal protein diets improve body composition and \n",
      "glucose control in adults with type 2 diabetes: A randomized \n",
      "trial\n",
      "Julianne G. Clina1, R. Drew Sayer1,3, Zhaoxing Pan2, Caroline W. Cohen3,\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(\"Preview of the first chunk:\", chunks[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenizing the Text with Different Tokenizers\n",
    "\n",
    "Before embedding, it's insightful to see how different tokenizers break up our text. Here, we use the tokenizer from the SentenceTransformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teilt Text in 128-Token große, nicht überlappende Chunks auf, optimiert für das Modell „paraphrase-multilingual-MiniLM-L12-v2“.\n",
    "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=128, model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total chunks: 120\n",
      "High and normal protein diets improve body composition and glucose control in adults with type 2 diabetes: A randomized trial Julianne G. Clina1, R. Drew Sayer1,3, Zhaoxing Pan2, Caroline W. Cohen3, Michael T. McDermott4, Victoria A. Catenacci4, Holly R. Wyatt1,5, James O. Hill1 1Department of Nutrition Sciences, University of Alabama at Birmingham 2Department of Pediatrics, University of Colorado Anschutz Medical Campus 3Department of Family and Community Medicine, University of Alabama at Birmingham\n"
     ]
    }
   ],
   "source": [
    "# Zerlegt alle Textstücke nochmals in 128-Token-Chunks und sammelt sie in einer Liste.\n",
    "token_split_texts = []\n",
    "for text in chunks:\n",
    "    token_split_texts += token_splitter.split_text(text)\n",
    "\n",
    "print(f\"\\nTotal chunks: {len(token_split_texts)}\")\n",
    "print(token_split_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0: ['<s>', '▁High', '▁and', '▁normal', '▁protein', '▁die', 'ts', '▁improve', '▁body', '▁composition', '▁and', '▁gluco', 'se', '▁control', '▁in', '▁adults', '▁with', '▁type', '▁2', '▁diabetes', ':', '▁A', '▁random', 'ized', '▁trial', '▁Julian', 'ne', '▁G', '.', '▁C', 'lina', '1', ',', '▁R', '.', '▁Drew', '▁Say', 'er', '1', ',', '3', ',', '▁Z', 'ha', 'ox', 'ing', '▁Pan', '2', ',', '▁Caroline', '▁W', '.', '▁Cohen', '3', ',', '▁Michael', '▁T', '.', '▁Mc', 'Der', 'm', 'ott', '4', ',', '▁Victoria', '▁A', '.', '▁Cate', 'na', 'cci', '4', ',', '▁Holl', 'y', '▁R', '.', '▁Wy', 'att', '1,5', ',', '▁James', '▁O', '.', '▁Hill', '1', '▁1', 'De', 'part', 'ment', '▁of', '▁Nutrition', '▁Science', 's', ',', '▁University', '▁of', '▁Alabama', '▁at', '▁Birmingham', '▁2', 'De', 'part', 'ment', '▁of', '▁Pediatri', 'cs', ',', '▁University', '▁of', '▁Colorado', '▁An', 'schutz', '▁Medical', '▁Campus', '▁3', 'De', 'part', 'ment', '▁of', '▁Family', '▁and', '▁Community', '▁Medicine', ',', '▁University', '▁of', '▁Alabama', '</s>']\n",
      "Chunk 1: ['<s>', '▁4', 'Di', 'vision', '▁of', '▁En', 'doc', 'rin', 'ology', ',', '▁Meta', 'bol', 'ism', '▁and', '▁Diabetes', ',', '▁University', '▁of', '▁Colorado', '▁School', '▁of', '▁Medicine', ',', '▁Aurora', ',', '▁Colorado', '▁5', 'An', 'schutz', '▁Health', '▁and', '▁Wellness', '▁Center', ',', '▁University', '▁of', '▁Colorado', '▁An', 'schutz', '▁Medical', '▁Campus', '▁Abstract', '▁Object', 'ive', ':', '▁Weight', '▁loss', '▁of', '▁', '≥', '10%', '▁improve', 's', '▁gluco', 'se', '▁control', '▁and', '▁may', '▁remit', '▁type', '▁2', '▁diabetes', '▁(', 'T', '2', 'D', ').', '▁High', '▁protein', '▁(', 'HP', ')', '▁die', 'ts', '▁are', '▁common', 'ly', '▁used', '▁for', '▁weight', '▁loss', ',', '▁but', '▁whether', '▁protein', '▁sources', ',', '▁especially', '▁red', '▁me', 'at', ',', '▁impact', '▁weight', '▁loss', '-', 'indu', 'ced', '▁T', '2', 'D', '▁management', '▁is', '▁un', 'know', 'n', '.', '▁This', '▁trial', '▁compared', '▁a', '▁HP', '▁die', 't', '▁including', '▁red', '▁me', 'at', '▁and', '▁normal', '▁protein', '▁(', 'NP', ')', '▁without', '▁red', '▁me', '</s>']\n",
      "Chunk 2: ['<s>', '▁weight', '▁loss', ',', '▁body', '▁composition', '▁changes', ',', '▁and', '▁gluco', 'se', '▁control', '▁in', '▁individuals', '▁with', '▁T', '2', 'D', '.', '▁Method', 's', ':', '▁106', '▁adults', '▁(', '80', '▁female', ')', '▁with', '▁T', '2', 'D', '▁consum', 'ed', '▁a', '▁HP', '▁(', '40%', '▁protein', ')', '▁die', 't', '▁with', '▁', '≥', '4', '▁week', 'ly', '▁ser', 'ving', 's', '▁of', '▁lean', '▁be', 'ef', '▁or', '▁a', '▁', 'NP', '▁(2', '1%', '▁protein', ')', '▁die', 't', '▁exclu', 'ding', '▁red', '▁me', 'at', '▁during', '▁a', '▁52', '-', 'week', '▁weight', '▁loss', '▁intervention', '.', '▁Body', '▁weight', ',', '▁body', '▁composition', ',', '▁and', '▁cardio', 'meta', 'bo', 'lic', '▁parameter', 's', '▁were', '▁measure', 'd', '▁before', '▁and', '▁after', '▁intervention', '.', '▁Results', ':', '▁Weight', '▁loss', '▁was', '▁not', '▁different', '▁between', '▁HP', '▁(', '−', '10', '.', '2', '±', '1.6', '▁kg', ')', '▁and', '▁', 'NP', '▁(', '−', '12', '.', '7', '±', '</s>']\n",
      "Chunk 3: ['<s>', '▁8', '▁kg', ',', '▁p', '=', '0.3', '36)', '.', '▁Both', '▁groups', '▁reduce', 'd', '▁fat', '▁mass', '▁and', '▁increased', '▁fat', '▁free', '▁mass', '▁percent', '.', '▁Hem', 'o', 'glob', 'in', '▁A', '1', 'c', ',', '▁gluco', 'se', ',', '▁insulin', ',', '▁insulin', '▁resist', 'ance', ',', '▁blood', '▁pressure', ',', '▁and', '▁tri', 'gly', 'ceri', 'des', '▁improve', 'd', '▁with', '▁no', '▁difference', 's', '▁between', '▁groups', '.', '▁Conclu', 'sions', ':', '▁The', '▁lack', '▁of', '▁observe', 'd', '▁effects', '▁of', '▁dieta', 'ry', '▁protein', '▁and', '▁red', '▁me', 'at', '▁consum', 'p', 'tion', '▁on', '▁weight', '▁loss', '▁and', '▁improve', 'd', '▁cardio', 'meta', 'bo', 'lic', '▁health', '▁suggest', '▁that', '▁achieve', 'd', '▁weight', '▁loss', '▁–', '▁rather', '▁than', '</s>']\n",
      "Chunk 4: ['<s>', '▁Conclu', 'sions', ':', '▁The', '▁lack', '▁of', '▁observe', 'd', '▁effects', '▁of', '▁dieta', 'ry', '▁protein', '▁and', '▁red', '▁me', 'at', '▁consum', 'p', 'tion', '▁on', '▁weight', '▁loss', '▁and', '▁improve', 'd', '▁cardio', 'meta', 'bo', 'lic', '▁health', '▁suggest', '▁that', '▁achieve', 'd', '▁weight', '▁loss', '▁–', '▁rather', '▁than', '▁die', 't', '▁composition', '▁–', '▁should', '▁be', '▁the', '▁principal', '▁target', '▁of', '▁dieta', 'ry', '▁intervention', 's', '▁for', '▁T', '2', 'D', '▁management', '.', '▁Contact', '▁Info', ':', '▁R', '.', '▁Drew', '▁Say', 'er', ',', '▁Community', '▁Health', '▁Services', '▁Building', '▁(', 'CH', '20)', '▁30', '7', 'B', ',', '▁9', '30', '▁20', 'th', '▁St', '▁S', ',', '▁Birmingham', ',', '▁AL', '▁35', '29', '4', ',', '▁say', 'erd', '@', 'u', 'ab', '.', 'edu', '.', '▁Author', '▁Con', 'tribution', ':', '▁JO', 'H', ',', '▁H', 'J', 'W', ',', '▁and', '▁R', 'DS', '▁conce', 'i', 'ved', '▁the', '▁research', '▁project', ';', '▁J', 'GC', '▁and', '</s>']\n",
      "Chunk 5: ['<s>', '▁conduct', 'ed', '▁the', '▁research', ';', '▁', 'ZP', '▁perform', 'ed', '▁the', '▁statistic', 'al', '▁analyse', 's', ';', '▁J', 'GC', '▁draft', 'ed', '▁the', '▁manu', 'script', '▁and', '▁R', 'DS', ',', '▁JO', 'H', ',', '▁H', 'J', 'W', ',', '▁R', 'DS', ',', '▁C', 'WC', ',', '▁M', 'TM', ',', '▁V', '▁AC', ',', '▁and', '▁', 'ZP', '▁provided', '▁critical', '▁feedback', '▁and', '▁edit', 's', '▁to', '▁the', '▁manu', 'script', '.', '▁All', '▁author', 's', '▁take', '▁responsibility', '▁for', '▁the', '▁final', '▁content', '▁of', '▁the', '▁manu', 'script', '.', '▁Clinic', 'al', '▁Tri', 'al', '▁Registr', 'ation', ':', '▁National', '▁Clinic', 'al', '▁Tri', 'al', '▁N', 'CT', '03', '83', '29', '33', '▁Dis', 'closure', ':', '▁All', '▁other', '▁author', 's', '▁declare', '▁no', '▁conflict', 's', '▁of', '▁interest', '.', '▁H', 'HS', '▁Public', '▁Access', '▁Author', '▁manu', 'script', '▁Ob', 'es', 'ity', '▁(', 'Sil', 'ver', '▁Spring', ')', '▁', '.', '▁Author', '▁manu', 'script', ';', '</s>']\n",
      "Chunk 6: ['<s>', '▁PM', 'C', '▁20', '24', '▁August', '▁01.', '▁Published', '▁in', '▁final', '▁edit', 'ed', '▁form', '▁as', ':', '▁Ob', 'es', 'ity', '▁(', 'Sil', 'ver', '▁Spring', ')', '▁', '.', '▁2023', '▁August', '▁;', '▁31', '(', '8)', ':', '▁2021', '–', '20', '30', '.', '▁do', 'i', ':10', '.', '100', '2/', 'oby', '.', '2', '38', '15', '.', '▁Author', '▁Manu', 'script', '▁Author', '▁Manu', 'script', '▁Author', '▁Manu', 'script', '▁Author', '▁Manu', 'script', '▁Keyword', 's', '▁obes', 'ity', ';', '▁weight', '▁loss', ';', '▁body', '▁composition', ';', '▁type', '▁2', '▁diabetes', '▁Introdu', 'ction', '▁Type', '▁2', '▁diabetes', '▁(', 'T', '2', 'D', ')', '▁affect', 's', '▁over', '▁30', '▁million', '▁adults', '▁in', '▁America', '▁and', '▁present', 's', '▁numerous', '▁public', '▁health', '▁challenges', '▁[', '▁1', ']', '.', '▁T', '2', 'D', '▁is', '▁a', '▁major', '▁risk', '▁factor', '▁for', '▁cardiovascular', '▁disease', '▁[', '▁2', ']', ',', '▁ki', 'dne', 'y', '▁disease', '▁[', '▁3', ',', '▁4', '</s>']\n",
      "Chunk 7: ['<s>', '▁a', 'mput', 'ation', '▁[', '▁5', '–', '7', ']', ',', '▁certain', '▁cancer', 's', '▁[', '▁8', ']', ',', '▁and', '▁blind', 'ness', '▁[', '▁9', ',', '▁10', ']', ',', '▁which', '▁results', '▁in', '▁a', '▁major', '▁cost', '▁burde', 'n', '▁to', '▁the', '▁health', 'care', '▁system', '▁[', '▁1', ']', '.', '▁The', '▁primary', '▁risk', '▁factor', '▁for', '▁T', '2', 'D', '▁is', '▁obes', 'ity', ',', '▁with', '▁the', '▁majority', '▁of', '▁those', '▁with', '▁T', '2', 'D', '▁having', '▁over', 'weight', '▁or', '▁obes', 'ity', '▁[', '▁11', ',', '▁12', ']', '.', '▁Ob', 'es', 'ity', '▁also', '▁increase', 's', '▁the', '▁risk', '▁of', '▁several', '▁other', '▁co', '-', 'mor', 'bid', '▁conditions', '▁including', '▁heart', '▁disease', '▁and', '▁stroke', '▁[', '11', ',', '▁13,', '▁14', ']', '.', '▁It', '▁has', '▁been', '▁demonstrat', 'ed', '▁that', '▁both', '▁T', '2', 'D', '▁and', '▁obes', 'ity', '▁can', '▁be', '▁treated', '▁with', '▁lifestyle', '</s>']\n",
      "Chunk 8: ['<s>', '▁increase', 's', '▁the', '▁risk', '▁of', '▁several', '▁other', '▁co', '-', 'mor', 'bid', '▁conditions', '▁including', '▁heart', '▁disease', '▁and', '▁stroke', '▁[', '11', ',', '▁13,', '▁14', ']', '.', '▁It', '▁has', '▁been', '▁demonstrat', 'ed', '▁that', '▁both', '▁T', '2', 'D', '▁and', '▁obes', 'ity', '▁can', '▁be', '▁treated', '▁with', '▁lifestyle', '▁modification', '.', '▁For', '▁example', ',', '▁in', '▁Di', 'R', 'ECT', '▁(', 'Dia', 'bete', 's', '▁Re', 'mission', '▁Clinic', 'al', '▁Tri', 'al', ')', '▁weight', '▁loss', '▁of', '▁10', '▁to', '▁15', '▁kg', '▁result', 'ed', '▁in', '▁the', '▁re', 'mission', '▁of', '▁T', '2', 'D', '▁in', '▁a', '▁majority', '▁of', '▁individuals', '▁who', '▁had', '▁been', '▁who', '▁had', '▁been', '▁diagnose', 'd', '▁with', '▁T', '2', 'D', '▁within', '▁the', '▁past', '▁6', '▁years', '.', '▁Ne', 'ar', 'ly', '▁9', '▁in', '▁10', '▁of', '▁individuals', '▁a', 'chie', 'ving', '▁more', '▁than', '▁15', '▁kg', '▁of', '▁weight', '▁loss', '▁remit', 'ted', '▁their', '▁T', '2', 'D', '</s>']\n",
      "Chunk 9: ['<s>', '▁]', '.', '▁While', '▁it', '▁is', '▁clear', '▁that', '▁weight', '▁loss', '▁is', '▁associated', '▁with', '▁improvement', 's', '▁in', '▁T', '2', 'D', ',', '▁the', '▁role', '▁of', '▁die', 't', '▁composition', '▁in', '▁the', '▁rever', 'sal', '▁of', '▁T', '2', 'D', '▁present', 's', '▁a', '▁gap', '▁in', '▁knowledge', '.', '▁High', 'er', '▁protein', '▁die', 'ts', '▁are', '▁an', '▁attractive', '▁target', '▁for', '▁lifestyle', '-', 'based', '▁intervention', 's', '▁for', '▁the', '▁treatment', '▁of', '▁T', '2', 'D', '.', '▁High', '▁protein', '▁die', 'ts', ',', '▁especially', '▁when', '▁combine', 'd', '▁with', '▁exercise', ',', '▁produce', '▁greater', '▁weight', '▁loss', '▁and', '▁prevent', '▁los', 'ses', '▁of', '▁fat', '▁free', '▁mass', '▁(', 'FF', 'M', ')', '▁compared', '▁to', '▁lower', '▁protein', '▁die', 'ts', '▁[', '▁16', '–18', ']', '.', '▁In', '▁pre', 'meno', 'paus', 'al', '▁women', '▁with', '▁obes', 'ity', '▁without', '▁diabetes', ',', '▁a', '▁high', '▁protein', '▁die', 't', '▁improve', 'd', '▁insulin', '▁', 'sensitiv', 'ity', '▁more', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Tokenisiert die ersten 10 Chunks auf 128 Tokens, wandelt IDs zurück zu lesbaren Tokens und zeigt sie an.\n",
    "model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "tokenized_chunks = []\n",
    "for i, text in enumerate(token_split_texts[:10]):\n",
    "    # Tokenize each chunk\n",
    "    encoded_input = model.tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    # Convert token IDs back to tokens\n",
    "    tokens = model.tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0].tolist())\n",
    "    tokenized_chunks.append(tokens)\n",
    "    print(f\"Chunk {i}: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0: ['[CLS]', 'High', 'and', 'normal', 'pro', '##te', '##in', 'die', '##ts', 'im', '##pro', '##ve', 'bo', '##dy', 'comp', '##osition', 'and', 'gl', '##uc', '##ose', 'con', '##trol', 'in', 'ad', '##ult', '##s', 'with', 'type', '2', 'di', '##abet', '##es', ':', 'A', 'ran', '##dom', '##ized', 'tri', '##al', 'Julian', '##ne', 'G', '.', 'Clin', '##a', '##1', ',', 'R', '.', 'Dre', '##w', 'Say', '##er', '##1', ',', '3', ',', 'Z', '##ha', '##ox', '##ing', 'Pan', '##2', ',', 'Caroline', 'W', '.', 'Cohen', '##3', ',', 'Michael', 'T', '.', 'Mc', '##Der', '##mo', '##tt', '##4', ',', 'Victoria', 'A', '.', 'Cat', '##ena', '##cc', '##i', '##4', ',', 'Holly', 'R', '.', 'Wy', '##att', '##1', ',', '5', ',', 'James', 'O', '.', 'Hill', '##1', '1', '##De', '##partment', 'of', 'Nut', '##rit', '##ion', 'Sciences', ',', 'University', 'of', 'Ala', '##ba', '##ma', 'at', 'Birmingham', '2', '##De', '##partment', 'of', 'Ped', '##iat', '##ric', '##s', ',', '[SEP]']\n",
      "Chunk 1: ['[CLS]', '4', '##Division', 'of', 'End', '##oc', '##rin', '##ology', ',', 'Met', '##abo', '##li', '##sm', 'and', 'Diabet', '##es', ',', 'University', 'of', 'Colorado', 'School', 'of', 'Med', '##ici', '##ne', ',', 'Aur', '##ora', ',', 'Colorado', '5', '##An', '##schutz', 'Health', 'and', 'Wellness', 'Center', ',', 'University', 'of', 'Colorado', 'Ans', '##ch', '##utz', 'Medical', 'Campus', 'Abstra', '##ct', 'Ob', '##ject', '##ive', ':', 'Wei', '##gh', '##t', 'los', '##s', 'of', '[UNK]', '%', 'im', '##pro', '##ves', 'gl', '##uc', '##ose', 'con', '##trol', 'and', 'may', 'rem', '##it', 'type', '2', 'di', '##abet', '##es', '(', 'T', '##2', '##D', ')', '.', 'High', 'pro', '##te', '##in', '(', 'H', '##P', ')', 'die', '##ts', 'are', 'comm', '##on', '##ly', 'use', '##d', 'for', 'wei', '##gh', '##t', 'los', '##s', ',', 'but', 'wh', '##ethe', '##r', 'pro', '##te', '##in', 'so', '##urce', '##s', ',', 'es', '##pe', '##cial', '##ly', 'red', 'me', '##at', ',', 'im', '##pa', '[SEP]']\n",
      "Chunk 2: ['[CLS]', 'wei', '##gh', '##t', 'los', '##s', ',', 'bo', '##dy', 'comp', '##osition', 'ch', '##ange', '##s', ',', 'and', 'gl', '##uc', '##ose', 'con', '##trol', 'in', 'individ', '##ual', '##s', 'with', 'T', '##2', '##D', '.', 'Metho', '##ds', ':', '106', 'ad', '##ult', '##s', '(', '80', 'fe', '##male', ')', 'with', 'T', '##2', '##D', 'con', '##sum', '##ed', 'a', 'H', '##P', '(', '40', '%', 'pro', '##te', '##in', ')', 'die', '##t', 'with', '[UNK]', 'we', '##ek', '##ly', 'serv', '##ings', 'of', 'le', '##an', 'be', '##ef', 'or', 'a', 'N', '##P', '(', '21', '%', 'pro', '##te', '##in', ')', 'die', '##t', 'ex', '##cl', '##udi', '##ng', 'red', 'me', '##at', 'dur', '##ing', 'a', '52', '-', 'we', '##ek', 'wei', '##gh', '##t', 'los', '##s', 'inter', '##vention', '.', 'Body', 'wei', '##gh', '##t', ',', 'bo', '##dy', 'comp', '##osition', ',', 'and', 'car', '##dio', '##met', '##abo', '##li', '##c', 'par', '##ameter', '[SEP]']\n",
      "Chunk 3: ['[CLS]', '8', 'kg', ',', 'p', '=', '0', '.', '33', '##6', ')', '.', 'Bot', '##h', 'gro', '##ups', 'red', '##uc', '##ed', 'fa', '##t', 'mass', 'and', 'in', '##cre', '##ase', '##d', 'fa', '##t', 'free', 'mass', 'per', '##cent', '.', 'Hem', '##ogl', '##obi', '##n', 'A', '##1', '##c', ',', 'gl', '##uc', '##ose', ',', 'ins', '##ulin', ',', 'ins', '##ulin', 'res', '##istan', '##ce', ',', 'blo', '##od', 'pre', '##ss', '##ure', ',', 'and', 'tri', '##gl', '##yce', '##rid', '##es', 'im', '##pro', '##ved', 'with', 'no', 'differ', '##ence', '##s', 'between', 'gro', '##ups', '.', 'Conc', '##lu', '##si', '##ons', ':', 'The', 'la', '##ck', 'of', 'ob', '##serv', '##ed', 'effe', '##cts', 'of', 'die', '##ta', '##ry', 'pro', '##te', '##in', 'and', 'red', 'me', '##at', 'con', '##sum', '##ption', 'on', 'wei', '##gh', '##t', 'los', '##s', 'and', 'im', '##pro', '##ved', 'car', '##dio', '##met', '##abo', '##li', '##c', 'he', '##alth', 'su', '##gge', '[SEP]']\n",
      "Chunk 4: ['[CLS]', 'Conc', '##lu', '##si', '##ons', ':', 'The', 'la', '##ck', 'of', 'ob', '##serv', '##ed', 'effe', '##cts', 'of', 'die', '##ta', '##ry', 'pro', '##te', '##in', 'and', 'red', 'me', '##at', 'con', '##sum', '##ption', 'on', 'wei', '##gh', '##t', 'los', '##s', 'and', 'im', '##pro', '##ved', 'car', '##dio', '##met', '##abo', '##li', '##c', 'he', '##alth', 'su', '##gge', '##st', 'that', 'ach', '##ie', '##ved', 'wei', '##gh', '##t', 'los', '##s', '–', 'rat', '##her', 'than', 'die', '##t', 'comp', '##osition', '–', 'sho', '##uld', 'be', 'the', 'pri', '##nc', '##ip', '##al', 'target', 'of', 'die', '##ta', '##ry', 'inter', '##vention', '##s', 'for', 'T', '##2', '##D', 'man', '##agement', '.', 'Con', '##tact', 'Info', ':', 'R', '.', 'Dre', '##w', 'Say', '##er', ',', 'Community', 'Health', 'Services', 'Bu', '##ilding', '(', 'CH', '##20', ')', '30', '##7', '##B', ',', '93', '##0', '20', '##th', 'St', 'S', ',', 'Birmingham', ',', 'AL', '35', '##29', '[SEP]']\n",
      "Chunk 5: ['[CLS]', 'con', '##du', '##ct', '##ed', 'the', 'res', '##earch', ';', 'Z', '##P', 'per', '##form', '##ed', 'the', 'stat', '##ist', '##ical', 'analys', '##es', ';', 'J', '##G', '##C', 'dra', '##fte', '##d', 'the', 'man', '##us', '##cr', '##ip', '##t', 'and', 'R', '##DS', ',', 'J', '##O', '##H', ',', 'H', '##J', '##W', ',', 'R', '##DS', ',', 'C', '##WC', ',', 'MT', '##M', ',', 'V', 'AC', ',', 'and', 'Z', '##P', 'pro', '##vide', '##d', 'c', '##rit', '##ical', 'fe', '##ed', '##back', 'and', 'edi', '##ts', 'to', 'the', 'man', '##us', '##cr', '##ip', '##t', '.', 'All', 'aut', '##hor', '##s', 'ta', '##ke', 'res', '##pon', '##sibil', '##ity', 'for', 'the', 'fin', '##al', 'con', '##tent', 'of', 'the', 'man', '##us', '##cr', '##ip', '##t', '.', 'Clin', '##ical', 'Tri', '##al', 'Registr', '##ation', ':', 'National', 'Clin', '##ical', 'Tri', '##al', 'N', '##CT', '##03', '##83', '##29', '##33', 'Dis', '##cl', '##os', '##ure', ':', '[SEP]']\n",
      "Chunk 6: ['[CLS]', 'PM', '##C', '202', '##4', 'August', '01', '.', 'Publ', '##ished', 'in', 'fin', '##al', 'edi', '##ted', 'form', 'as', ':', 'Ob', '##esi', '##ty', '(', 'Silver', 'Spring', ')', '.', '202', '##3', 'August', ';', '31', '(', '8', ')', ':', '2021', '–', '2030', '.', 'do', '##i', ':', '10', '.', '100', '##2', '/', 'ob', '##y', '.', '23', '##81', '##5', '.', 'Author', 'Man', '##us', '##cr', '##ip', '##t', 'Author', 'Man', '##us', '##cr', '##ip', '##t', 'Author', 'Man', '##us', '##cr', '##ip', '##t', 'Author', 'Man', '##us', '##cr', '##ip', '##t', 'Key', '##word', '##s', 'ob', '##esi', '##ty', ';', 'wei', '##gh', '##t', 'los', '##s', ';', 'bo', '##dy', 'comp', '##osition', ';', 'type', '2', 'di', '##abet', '##es', 'Int', '##rod', '##uc', '##tion', 'Typ', '##e', '2', 'di', '##abet', '##es', '(', 'T', '##2', '##D', ')', 'af', '##fe', '##cts', 'over', '30', 'million', 'ad', '##ult', '##s', 'in', 'America', '[SEP]']\n",
      "Chunk 7: ['[CLS]', 'am', '##put', '##ation', '[', '5', '–', '7', ']', ',', 'ce', '##rt', '##ain', 'can', '##cer', '##s', '[', '8', ']', ',', 'and', 'blind', '##ness', '[', '9', ',', '10', ']', ',', 'which', 'resul', '##ts', 'in', 'a', 'ma', '##jor', 'co', '##st', 'bu', '##rd', '##en', 'to', 'the', 'he', '##alth', '##car', '##e', 'system', '[', '1', ']', '.', 'The', 'prim', '##ary', 'risk', 'fac', '##tor', 'for', 'T', '##2', '##D', 'is', 'ob', '##esi', '##ty', ',', 'with', 'the', 'ma', '##jor', '##ity', 'of', 'th', '##ose', 'with', 'T', '##2', '##D', 'ha', '##vin', '##g', 'over', '##wei', '##gh', '##t', 'or', 'ob', '##esi', '##ty', '[', '11', ',', '12', ']', '.', 'Ob', '##esi', '##ty', 'also', 'in', '##cre', '##ases', 'the', 'risk', 'of', 'se', '##ver', '##al', 'other', 'co', '-', 'mor', '##bi', '##d', 'con', '##dition', '##s', 'incl', '##udi', '##ng', 'he', '##art', 'dis', '##ea', '##se', 'and', '[SEP]']\n",
      "Chunk 8: ['[CLS]', 'in', '##cre', '##ases', 'the', 'risk', 'of', 'se', '##ver', '##al', 'other', 'co', '-', 'mor', '##bi', '##d', 'con', '##dition', '##s', 'incl', '##udi', '##ng', 'he', '##art', 'dis', '##ea', '##se', 'and', 'st', '##ro', '##ke', '[', '11', ',', '13', ',', '14', ']', '.', 'It', 'has', 'been', 'demonstr', '##ated', 'that', 'bot', '##h', 'T', '##2', '##D', 'and', 'ob', '##esi', '##ty', 'can', 'be', 'tre', '##ated', 'with', 'life', '##style', 'modif', '##ication', '.', 'For', 'ex', '##amp', '##le', ',', 'in', 'Di', '##R', '##EC', '##T', '(', 'Diabet', '##es', 'Rem', '##ission', 'Clin', '##ical', 'Tri', '##al', ')', 'wei', '##gh', '##t', 'los', '##s', 'of', '10', 'to', '15', 'kg', 'resul', '##ted', 'in', 'the', 'rem', '##ission', 'of', 'T', '##2', '##D', 'in', 'a', 'ma', '##jor', '##ity', 'of', 'individ', '##ual', '##s', 'who', 'had', 'been', 'who', 'had', 'been', 'di', '##agnose', '##d', 'with', 'T', '##2', '##D', 'with', '[SEP]']\n",
      "Chunk 9: ['[CLS]', ']', '.', 'Wh', '##ile', 'it', 'is', 'cle', '##ar', 'that', 'wei', '##gh', '##t', 'los', '##s', 'is', 'ass', '##oc', '##iat', '##ed', 'with', 'im', '##pro', '##ve', '##ments', 'in', 'T', '##2', '##D', ',', 'the', 'ro', '##le', 'of', 'die', '##t', 'comp', '##osition', 'in', 'the', 're', '##vers', '##al', 'of', 'T', '##2', '##D', 'pre', '##sent', '##s', 'a', 'ga', '##p', 'in', 'know', '##led', '##ge', '.', 'High', '##er', 'pro', '##te', '##in', 'die', '##ts', 'are', 'an', 'attra', '##ct', '##ive', 'target', 'for', 'life', '##style', '-', 'based', 'inter', '##vention', '##s', 'for', 'the', 'tre', '##at', '##ment', 'of', 'T', '##2', '##D', '.', 'High', 'pro', '##te', '##in', 'die', '##ts', ',', 'es', '##pe', '##cial', '##ly', 'when', 'com', '##bin', '##ed', 'with', 'ex', '##er', '##cis', '##e', ',', 'produ', '##ce', 'great', '##er', 'wei', '##gh', '##t', 'los', '##s', 'and', 'pre', '##vent', 'los', '##ses', 'of', 'fa', '##t', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "#Jetzt wird ein Modell verwendet, das speziell für deutsche Texte optimiert ist, dadurch werden die Tokenisierung \n",
    "# und spätere Embeddings besser auf Deutsch angepasst.\n",
    "model_name = \"Sahajtomar/German-semantic\"\n",
    "model = SentenceTransformer(model_name)\n",
    "tokenized_chunks = []\n",
    "for i, text in enumerate(token_split_texts[:10]):\n",
    "    # Tokenize each chunk\n",
    "    encoded_input = model.tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    # Convert token IDs back to tokens\n",
    "    tokens = model.tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0].tolist())\n",
    "    tokenized_chunks.append(tokens)\n",
    "    print(f\"Chunk {i}: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating Embeddings for Each Chunk\n",
    "\n",
    "Now we convert each text chunk into a numerical embedding that captures its semantic meaning. These embeddings will be used for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14922/4140566380.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")\n"
     ]
    }
   ],
   "source": [
    "# Wandelt alle Token-Chunks direkt in Embeddings um und speichert sie als NumPy-Array.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "chunk_embeddings = model.encode(token_split_texts, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "## 5. Building a FAISS Vector Store\n",
    "\n",
    "FAISS is a powerful library for efficient similarity search. Here, we build an index from our embeddings. Remember, FAISS only stores the numerical vectors so we must keep our original text mapping separately.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Dimensionen pro Embedding: 1024\n"
     ]
    }
   ],
   "source": [
    "#Der Code gibt die Anzahl der Dimensionen jedes Embedding-Vektors aus – also die Vektorgröße.\n",
    "# chunk_embeddings = [\n",
    "#  [0.12, -0.88, 0.45, 0.77, ..., 0.33],  # ➔ Chunk 1 (ein Embedding)\n",
    "#  [0.34, -0.22, 0.78, 0.11, ..., -0.50], # ➔ Chunk 2\n",
    "#  [0.91,  0.05, 0.12, -0.99, ..., 0.71], # ➔ Chunk 3\n",
    "#]\n",
    "# shape[1]-> Grösse der zweiten Dimension, also die Anzahl der Dimensionen jedes Embedding-Vektors.\n",
    "d = chunk_embeddings.shape[1]\n",
    "print(f\"Anzahl der Dimensionen pro Embedding: {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings in FAISS index: 120\n"
     ]
    }
   ],
   "source": [
    "# Der Code erstellt einen FAISS-Index für effiziente Ähnlichkeitssuche, fügt Embeddings hinzu \n",
    "# und gibt die Anzahl der gespeicherten Vektoren aus.\n",
    "# L2-Distanz (euklidische Distanz) verwendet\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(chunk_embeddings)\n",
    "print(\"Number of embeddings in FAISS index:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Der Code speichert den FAISS-Index und die zugehörigen Text-Chunks lokal ab, um sie später wiederverwenden zu können.\n",
    "faiss.write_index(index, \"faiss/faiss_index.index\")\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "#Musste hier zuerst faiss folder erstellen!\n",
    "#Der Code lädt den gespeicherten FAISS-Index und die Chunk-Texte wieder aus Dateien und prüft, \n",
    "#ob die Anzahl der geladenen Chunks mit den ursprünglichen übereinstimmt.\n",
    "index_2 = faiss.read_index(\"faiss/faiss_index.index\")\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"rb\") as f:\n",
    "    token_split_texts_2 = pickle.load(f)\n",
    "print(len(token_split_texts_2))\n",
    "print(len(token_split_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Projecting Embeddings with UMAP\n",
    "\n",
    "To visualize high-dimensional embeddings, we use UMAP to project them into 2D space. You can project both the entire dataset and individual query embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Fit UMAP on the full dataset embeddings\n",
    "# Der Code trainiert UMAP auf den Embeddings, um sie auf 2D zu reduzieren, und definiert eine Funktion, \n",
    "# die beliebige Embeddings mithilfe dieser UMAP-Transformation in zwei Dimensionen projiziert – z. B. für die Visualisierung.\n",
    "umap_transform = umap.UMAP(random_state=0, transform_seed=0).fit(chunk_embeddings)\n",
    "\n",
    "def project_embeddings(embeddings, umap_transform):\n",
    "    \"\"\"\n",
    "    Project a set of embeddings using a pre-fitted UMAP transform.\n",
    "    \"\"\"\n",
    "    umap_embeddings = np.empty((len(embeddings), 2))\n",
    "    for i, embedding in enumerate(tqdm.tqdm(embeddings, desc=\"Projecting Embeddings\")):\n",
    "        umap_embeddings[i] = umap_transform.transform([embedding])\n",
    "    return umap_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projecting Embeddings:   0%|          | 0/120 [00:00<?, ?it/s]/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "Projecting Embeddings:   1%|          | 1/120 [00:01<03:49,  1.93s/it]/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "Projecting Embeddings:  37%|███▋      | 44/120 [00:02<00:02, 30.04it/s]/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "Projecting Embeddings:  60%|██████    | 72/120 [00:02<00:00, 52.38it/s]/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "Projecting Embeddings:  93%|█████████▎| 112/120 [00:02<00:00, 91.64it/s]/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "Projecting Embeddings: 100%|██████████| 120/120 [00:02<00:00, 53.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected dataset embeddings shape: (120, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Project the entire dataset embeddings\n",
    "projected_dataset_embeddings = project_embeddings(chunk_embeddings, umap_transform)\n",
    "print(\"Projected dataset embeddings shape:\", projected_dataset_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Querying the Vector Store and Projecting Results\n",
    "\n",
    "We now define a retrieval function that takes a text query, embeds it, and searches our FAISS index for similar documents. We then project these result embeddings with UMAP.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Die Funktion retrieve(query, k=5) berechnet ein Embedding für die Anfrage, sucht die k ähnlichsten Chunks im \n",
    "#FAISS-Index und gibt die zugehörigen Texte, Embeddings und Distanzen zurück.\n",
    "def retrieve(query, k=5):\n",
    "    \"\"\"\n",
    "    Retrieve the top k similar text chunks and their embeddings for a given query.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    retrieved_texts = [chunks[i] for i in indices[0]]\n",
    "    retrieved_embeddings = np.array([chunk_embeddings[i] for i in indices[0]])\n",
    "    return retrieved_texts, retrieved_embeddings, distances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmachine learning algorithms\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m results, result_embeddings, distances \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrieved document preview:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(results[\u001b[38;5;241m0\u001b[39m][:\u001b[38;5;241m300\u001b[39m])\n",
      "Cell \u001b[0;32mIn[18], line 9\u001b[0m, in \u001b[0;36mretrieve\u001b[0;34m(query, k)\u001b[0m\n\u001b[1;32m      7\u001b[0m query_embedding \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode([query], convert_to_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m distances, indices \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39msearch(query_embedding, k)\n\u001b[0;32m----> 9\u001b[0m retrieved_texts \u001b[38;5;241m=\u001b[39m [\u001b[43mchunks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m     10\u001b[0m retrieved_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([chunk_embeddings[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices[\u001b[38;5;241m0\u001b[39m]])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retrieved_texts, retrieved_embeddings, distances[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "query = \"machine learning algorithms\"\n",
    "results, result_embeddings, distances = retrieve(query, k=2)\n",
    "print(\"Retrieved document preview:\")\n",
    "print(results[0][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Project the result embeddings\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m projected_result_embeddings \u001b[38;5;241m=\u001b[39m project_embeddings(\u001b[43mresult_embeddings\u001b[49m, umap_transform)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Also embed and project the original query for visualization\u001b[39;00m\n\u001b[1;32m      5\u001b[0m query_embedding \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode([query], convert_to_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# Project the result embeddings\n",
    "projected_result_embeddings = project_embeddings(result_embeddings, umap_transform)\n",
    "\n",
    "# Also embed and project the original query for visualization\n",
    "query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "project_original_query = project_embeddings(query_embedding, umap_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizing the Results\n",
    "\n",
    "Finally, we create a scatter plot to visualize the entire dataset, the retrieved results, and the original query in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'projected_result_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Scatter plots\u001b[39;00m\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(projected_dataset_embeddings[:, \u001b[38;5;241m0\u001b[39m], projected_dataset_embeddings[:, \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m      9\u001b[0m             s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(\u001b[43mprojected_result_embeddings\u001b[49m[:, \u001b[38;5;241m0\u001b[39m], projected_result_embeddings[:, \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     11\u001b[0m             s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, facecolors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m, edgecolors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mg\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResults\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(project_original_query[:, \u001b[38;5;241m0\u001b[39m], project_original_query[:, \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     13\u001b[0m             s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal Query\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# If results is a list of texts, iterate directly\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'projected_result_embeddings' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def shorten_text(text, max_length=15):\n",
    "    \"\"\"Shortens text to max_length and adds an ellipsis if shortened.\"\"\"\n",
    "    return (text[:max_length] + '...') if len(text) > max_length else text\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Scatter plots\n",
    "plt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1],\n",
    "            s=10, color='gray', label='Dataset')\n",
    "plt.scatter(projected_result_embeddings[:, 0], projected_result_embeddings[:, 1],\n",
    "            s=100, facecolors='none', edgecolors='g', label='Results')\n",
    "plt.scatter(project_original_query[:, 0], project_original_query[:, 1],\n",
    "            s=150, marker='X', color='r', label='Original Query')\n",
    "\n",
    "# If results is a list of texts, iterate directly\n",
    "for i, text in enumerate(results):\n",
    "    if i < len(projected_result_embeddings):\n",
    "        plt.annotate(shorten_text(text),\n",
    "                     (projected_result_embeddings[i, 0], projected_result_embeddings[i, 1]),\n",
    "                     fontsize=8)\n",
    "\n",
    "# Annotate the original query point\n",
    "original_query_text = 'Original Query Text'  # Replace with your actual query text if needed\n",
    "plt.annotate(shorten_text(original_query_text),\n",
    "             (project_original_query[0, 0], project_original_query[0, 1]),\n",
    "             fontsize=8)\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('Hogwarts')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach Retrieved Results to LLM\n",
    "\n",
    "- Combine learnings from week 1 with approach from this week to inject your data into prompts and create a simple question answering system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#🪄 Schritt 1: Frage stellen + relevante Text-Chunks abrufen\n",
    "\n",
    "def retrieve_context(query, k=3):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    return [chunks[i] for i in indices[0] if i < len(chunks)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#🪄 Schritt 2: Prompt zusammenbauen\n",
    "def build_prompt(query, context_chunks):\n",
    "    context = \"\\n\\n\".join(context_chunks)\n",
    "    return f\"\"\"\n",
    "Answer the following question based only on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Importiere Module\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 2. Lade API-Key\n",
    "load_dotenv()\n",
    "key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# 3. Erstelle den Client GLOBAL\n",
    "client = OpenAI(api_key=key)\n",
    "\n",
    "# 4. Definiere deine Funktion\n",
    "def answer_question(query):\n",
    "    \"\"\"\n",
    "    Sendet die Nutzerfrage an OpenAI GPT, mit Kontext.\n",
    "    \"\"\"\n",
    "    context_chunks = retrieve_context(query)\n",
    "    prompt = build_prompt(query, context_chunks)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document does not provide any specific information about machine learning.\n"
     ]
    }
   ],
   "source": [
    "query = \"What does the document say about machine learning?\"\n",
    "answer = answer_question(query)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise has been shown to improve brain function by increasing blood flow and oxygenation to the brain, promoting the growth of new brain cells, and enhancing cognitive function such as memory and decision-making.\n"
     ]
    }
   ],
   "source": [
    "answerNew = answer_question(\"What are the effects of exercise on brain function?\")\n",
    "print(answerNew)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
